{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# pip install \"ragas>=0.3\" datasets pandas google-generativeai langchain-community\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, google.generativeai as genai\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"     # or set in env\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
   ],
   "id": "30904d9b8483e5cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.llms import ChatGoogleGenerativeAI\n",
    "from langchain_community.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# LLM-as-judge (deterministic)\n",
    "evaluator_llm = LangchainLLMWrapper(\n",
    "    ChatGoogleGenerativeAI(model=\"gemini-2.0-pro\", temperature=0.0)\n",
    ")\n",
    "# Embeddings for Ragas metrics\n",
    "evaluator_emb = LangchainEmbeddingsWrapper(\n",
    "    GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
    ")\n"
   ],
   "id": "4434f8d5b21b9d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def make_ragas_dataset(sources, summaries, questions=None, references=None):\n",
    "    rows = []\n",
    "    for i, (src, pred) in enumerate(zip(sources, summaries)):\n",
    "        rec = {\"retrieved_contexts\": [src], \"response\": pred}\n",
    "        if questions is not None:  rec[\"question\"]  = questions[i]\n",
    "        if references is not None: rec[\"reference\"] = references[i]\n",
    "        rows.append(rec)\n",
    "    return Dataset.from_list(rows)\n"
   ],
   "id": "1778fef7a037dded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    summarization_score,\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "def run_ragas_eval(ds,\n",
    "                   conciseness_coeff: float = 0.4,\n",
    "                   use_correctness: bool = True,\n",
    "                   use_relevancy: bool = True):\n",
    "    metrics = [faithfulness, summarization_score(use_conciseness=True, coeff=conciseness_coeff)]\n",
    "    if use_correctness and \"reference\" in ds.column_names: metrics.append(answer_correctness)\n",
    "    if use_relevancy and \"question\" in ds.column_names:   metrics.append(answer_relevancy)\n",
    "\n",
    "    res = evaluate(ds, metrics=metrics, llm=evaluator_llm, embeddings=evaluator_emb)\n",
    "    df = res.to_pandas()\n",
    "    for col in [\"answer_correctness\", \"answer_relevancy\"]:\n",
    "        if col not in df.columns: df[col] = 0.0\n",
    "    return df\n"
   ],
   "id": "34262c31020bf5d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def blended_score(df: pd.DataFrame,\n",
    "                  w_faith=0.50, w_sum=0.30, w_corr=0.15, w_rel=0.05):\n",
    "    for c in [\"faithfulness\",\"summarization_score\",\"answer_correctness\",\"answer_relevancy\"]:\n",
    "        if c in df.columns: df[c] = df[c].clip(0,1)\n",
    "        else: df[c] = 0.0\n",
    "    total = max(w_faith + w_sum + w_corr + w_rel, 1e-9)\n",
    "    df[\"reliability_score\"] = (\n",
    "        w_faith*df[\"faithfulness\"] +\n",
    "        w_sum*df[\"summarization_score\"] +\n",
    "        w_corr*df[\"answer_correctness\"] +\n",
    "        w_rel*df[\"answer_relevancy\"]\n",
    "    ) / total\n",
    "    return df\n"
   ],
   "id": "ca4304d774152795"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_gates(df, min_faith=0.80, min_blended=0.75):\n",
    "    df[\"pass_faithfulness\"] = df[\"faithfulness\"] >= min_faith\n",
    "    df[\"pass_overall\"] = df[\"reliability_score\"] >= min_blended\n",
    "    return df\n"
   ],
   "id": "ef89eb260df26ef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your data\n",
    "sources   = [source_text_1, source_text_2]\n",
    "summaries = [summary_1, summary_2]                # from Gemini 2.5 (generator)\n",
    "questions = [\"Summarize the financial document\"]*len(sources)   # or per-doc\n",
    "references = [gold_1, gold_2]      # optional; omit if not available\n",
    "\n",
    "ds  = make_ragas_dataset(sources, summaries, questions, references)\n",
    "df  = run_ragas_eval(ds, conciseness_coeff=0.4, use_correctness=True, use_relevancy=True)\n",
    "df  = blended_score(df, w_faith=0.50, w_sum=0.30, w_corr=0.15, w_rel=0.05)\n",
    "df  = apply_gates(df, min_faith=0.80, min_blended=0.75)\n",
    "print(df[[\"faithfulness\",\"summarization_score\",\"answer_correctness\",\"answer_relevancy\",\"reliability_score\",\"pass_faithfulness\",\"pass_overall\"]])\n"
   ],
   "id": "564d83b08ad991e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dspy\n",
    "\n",
    "class RagasReliabilityMetric:\n",
    "    def __init__(self, w=(0.50,0.30,0.15,0.05), conciseness_coeff=0.4):\n",
    "        self.w = w\n",
    "        self.conciseness_coeff = conciseness_coeff\n",
    "\n",
    "    def __call__(self, example, pred) -> float:\n",
    "        ds = make_ragas_dataset(\n",
    "            [example[\"source\"]],\n",
    "            [pred],\n",
    "            [example.get(\"question\",\"Summarize the financial document\")],\n",
    "            [example[\"reference\"]] if \"reference\" in example else None,\n",
    "        )\n",
    "        df = run_ragas_eval(ds, conciseness_coeff=self.conciseness_coeff,\n",
    "                            use_correctness=(\"reference\" in example), use_relevancy=True)\n",
    "        df = blended_score(df, *self.w)\n",
    "        return float(df[\"reliability_score\"].iloc[0])\n",
    "\n",
    "# Define your summarizer signature and LM for generation (Gemini 2.5)\n",
    "class Summarize(dspy.Signature):\n",
    "    \"\"\"Summarize the financial document.\"\"\"\n",
    "    source = dspy.InputField()\n",
    "    summary = dspy.OutputField()\n",
    "\n",
    "lm = dspy.LM(\"google/gemini-2.5-pro\", temperature=0.2)  # adapt to your env\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "generate_summary = dspy.Predict(Summarize)\n",
    "devset = [{\"source\": source_text_1, \"reference\": gold_1, \"question\":\"Summarize the financial document\"}]\n",
    "\n",
    "metric = RagasReliabilityMetric()\n",
    "tp = dspy.teleprompt.BootstrapFewShotWithRandomSearch(metric=metric, num_trials=10, max_bootstrapped_demos=6)\n",
    "optimized_program = tp.compile(generate_summary, trainset=devset)\n"
   ],
   "id": "2d344ffb001d84e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
